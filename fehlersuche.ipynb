{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import logging as log\n",
    "import src.config as config\n",
    "import sys\n",
    "import src.utilities as ut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## Set environmental values #########################################################\n",
    "#############################################################################################################################################\n",
    "\n",
    "log.basicConfig(\n",
    "    format='%(asctime)s -- %(levelname)s -- %(message)s', \n",
    "    datefmt='%d.%m.%Y %H:%M:%S', \n",
    "    level=log.INFO,\n",
    "    encoding='utf-8',\n",
    "    handlers=[\n",
    "        log.FileHandler(f\"logs/{dt.date.today().strftime('%y_%m_%d')}_einfach-bauen.log\"),\n",
    "        log.StreamHandler()]\n",
    "    )\n",
    "\n",
    "\n",
    "## Dateipfade\n",
    "\n",
    "# Datenbanken\n",
    "dir_db = './eb-data/database'\n",
    "\n",
    "# Ordner für Auswertungen\n",
    "dir_results = './eb-data/Results'\n",
    "\n",
    "### Rohdaten (Dropbox, Archiv & co)\n",
    "## 1) Energy Monitoring\n",
    "em_dropbox = r'\\\\nas.ads.mwn.de\\tuar\\l15\\private\\DATA\\FORSCHUNG\\04_Projekte\\2021\\Einfach_Bauen_3\\Daten\\1_rohdaten\\EM\\RmCU'\n",
    "\n",
    "## 2) Tinkerforge Dropbox Sync\n",
    "tf_dropbox = os.path.join(r'\\\\nas.ads.mwn.de','tuar','l15','private','DATA','FORSCHUNG','04_Projekte','2021','Einfach_Bauen_3','Daten','1_rohdaten')\n",
    "\n",
    "## 3) Archiv: Daten vor September (ohne Dropbox-Sync)\n",
    "tf_archive = r'\\\\nas.ads.mwn.de\\tuar\\l15\\private\\DATA\\FORSCHUNG\\04_Projekte\\2021\\Einfach_Bauen_3\\Daten\\ARCHIV\\1_rohdaten'\n",
    "\n",
    "# Kürzel der Messdaten\n",
    "buid ={'LB':'Leichtbetonhaus','MH':'Massivholzhaus','MW':'Ziegelhaus', 'WD':'Wetterstation', 'PM':'Pyranometer'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################___Functions___###############################################################\n",
    "#############################################################################################################################################\n",
    "\n",
    "def load_tf_file(path, nrows=None, debug = False):\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        engine='python',\n",
    "        encoding= 'latin-1',\n",
    "        sep=';',\n",
    "        nrows=nrows,\n",
    "        na_values=['-',' ','#N/V','#NV'],\n",
    "        decimal=',',\n",
    "        index_col=False)\n",
    "    \n",
    "    df = df[df.iloc[:, 0] != df.columns[0]].copy()\n",
    "\n",
    "    if len(df) < 1:\n",
    "        return \n",
    "\n",
    "    not_empty_cols = [(col, df.columns.get_loc(col)) for col in df.replace(' ', np.NaN).filter(like='Unnamed:').dropna(axis=1).columns]\n",
    "    cols = df.columns.to_list()\n",
    "    for not_empty_col in not_empty_cols:\n",
    "        i = not_empty_col[1]\n",
    "        j = not_empty_col[1]\n",
    "        while True:\n",
    "            j+=1\n",
    "            if len(df.iloc[:,j].dropna()) == 0:\n",
    "                break\n",
    "        delta = (j - not_empty_col[1])\n",
    "        for n in range(delta):\n",
    "            cols[i+n],cols[i+n+1] = cols[i+n+1],cols[i+n]\n",
    "            if debug:\n",
    "                print(f'{cols[i+n]} <-> {cols[i+n+1]}')\n",
    "        df.columns = cols\n",
    "\n",
    "    df.drop(list(df.filter(like='named').columns), axis=1, inplace=True)\n",
    "    if '-->Extra-Sensors-->' in df.columns:\n",
    "        df.drop(['-->Extra-Sensors-->'], axis=1, inplace=True)\n",
    "    df.columns = df.columns.str.replace('HM', 'MH')\n",
    "    df.columns = df.columns.str.replace('(Â°C)', '(°C)', regex=False)\n",
    "    df.columns = df.columns.str.replace('[Black Globe] ', '', regex=False)\n",
    "    df.columns = df.columns.str.replace(f'[LB_Black Globe] ', '', regex=False)\n",
    "    df.columns = df.columns.str.replace(f'[MW_Black Globe] ', '', regex=False)\n",
    "    df.columns = df.columns.str.replace(f'[MH_Black Globe] ', '', regex=False)\n",
    "    df.columns = df.columns.str.replace(f'[HM_Black Globe] ', '', regex=False)\n",
    "    df.columns = df.columns.str.replace(f'[LB_Black Globe_Metal] ', '', regex=False)\n",
    "\n",
    "    df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "\n",
    "    if len(df)<1:\n",
    "        return\n",
    "    else:\n",
    "        drop=[]\n",
    "        for c, col in enumerate(df.columns):\n",
    "            if col == 'Date' or col == 'Datum':\n",
    "                date=col\n",
    "                drop.append(col)\n",
    "            if col == 'Time' or col =='Uhrzeit':\n",
    "                time=col\n",
    "                drop.append(col)\n",
    "\n",
    "        df.insert(0,'Datetime',pd.to_datetime(df[date] +' '+ df[time],dayfirst=True))\n",
    "        df.drop(drop,axis=1,inplace=True)\n",
    "\n",
    "        if df.Datetime.is_unique != True:\n",
    "            df.drop_duplicates(ignore_index=True, inplace=True, keep='last')\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        df.set_index('Datetime',inplace=True)\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send = False \n",
    "#OverwriteDatabase = ['LB']\n",
    "#skip = ['MW', 'MH', 'PM', 'WD']\n",
    "#buid = ['LB','MW', 'MH', 'PM', 'WD']\n",
    "\n",
    "def tinkerforge_update(send=True, OverwriteDatabase=[], skip=[]):\n",
    "    log.info(f'------ Starte TinkerForge ------')\n",
    "    ## Erstelle Pfade zu den tinkerforge Datenbanken\n",
    "    db = {bui : os.path.join(dir_db,bui,'{}_tf_raw.csv'.format(bui)) for bui in buid}\n",
    "    ## Falls der Datenbank-Ordner noch nicht existiert, erstelle ihn.\n",
    "    for bui in db:\n",
    "        if os.path.isdir(os.path.dirname(db[bui])) == False:\n",
    "            os.makedirs(os.path.dirname(db[bui]))\n",
    "\n",
    "    files = {}                                                          #dict with path to archived datasheets\n",
    "    master_df={}                                                        #dict with database as DataFrame\n",
    "\n",
    "\n",
    "    log.info(f'Suche vorhandene Datenbank.')\n",
    "    for bui in buid:                                                        # Starte Import Datensheet. Iteration über die Einheiten (LB, MH, MW, PM, WD)\n",
    "        if bui not in skip:                                                 # Skippe benutzerdefinierte Varianten.\n",
    "            tf_drop_bui = os.path.join(tf_dropbox,bui)                      # Konstruiere den Pfad zum Dropbox Ordner der aktuellen Einheit\n",
    "            if os.path.isfile(db[bui]) and bui not in OverwriteDatabase:    # Hysterese: Wenn Datenbank schon vorhanden ist und Bauweise nicht in der Liste der zu überschreibenden Bauweisen ist, dann öffne die Vorhandene Datenbank\n",
    "                master_df[bui] = pd.read_csv(db[bui], low_memory=False, index_col='Datetime')   # Öffne die vorhandene Datenbank aus dem Ordner\n",
    "                log.info(f'{bui}: Vorhandene Datenbank geöffnet!')\n",
    "\n",
    "                # Suche nach neuen Datensätzen in der Dropbox\n",
    "                dropbox_files = {}\n",
    "                offline = {}\n",
    "                notification = {}\n",
    "\n",
    "                dropbox_path = os.path.join(tf_dropbox, bui)    # Konstruiere den Dateipfad zum DropboxOrdner der aktuellen Einheit\n",
    "                dropbox_files[bui] = [os.path.join(dropbox_path, fn) for fn in os.listdir(dropbox_path) if fn.endswith(f'{bui}.csv') ]  # Konstruiere die Pfade zu den einzelnen Datensheets\n",
    "                dropbox_files[bui].sort(key=lambda x: os.path.getmtime(x))  # sortiere die Dateipfade nach deren letztem Bearbeitungsdatum\n",
    "\n",
    "                last_day = pd.to_datetime(os.path.basename(dropbox_files[bui][-1]).rsplit('_',1)[0],format='%Y_%m_%d')  # Extrahiere das Datum der neusten Datei im DropBox-Ordner aus deren Dateiname\n",
    "                last_day_soll = dt.datetime.today().date()-dt.timedelta(1)                                              # Konstruiere das Soll-Datum, sprich gestern\n",
    "\n",
    "                if last_day.date() == dt.date.today():                                                                  # Vergleiche das Datum der neusten Datei mit dem Datum von Heute\n",
    "                    last_file = dropbox_files[bui].pop(-1)                                                              # Überspriche die Datei von Heute, da so noch fortgeschrieben wird.\n",
    "                    log.info(f'{bui}: Datei von Heute ({os.path.basename(last_file)}) wird übersprungen!')\n",
    "                if last_day.date() < last_day_soll:                                                                     # Wenn die neuste Datei mehr als einen tag zurück liegt funktioniert der Dropbox Abgleich nicht.\n",
    "                    offline[bui] = f'{buid_long[bui]} liefert seit {last_day.to_pydatetime().strftime(\"%d.%m.%Y\")} ({((dt.datetime.today()-last_day.to_pydatetime()).days)} Tage(n)) keine neuen Daten mehr.'   # Erstelle eine Benachrichtigung\n",
    "                    log.info(offline[bui])\n",
    "\n",
    "                log.info(f'{bui}: Durchsuche Dropbox nach neuen Datensätzen.')\n",
    "\n",
    "                df1 = []                                                                # Definiere eine Liste in der die gefundenen neuen Datensätze gespeichert werden.\n",
    "                n_files = len(dropbox_files[bui])                                       # Zähle alle gefundenen Datensätze (Für die Fortschrittsanzeige während des Ladens)\n",
    "                for n, file in enumerate(dropbox_files[bui]):                           # Iteration über alle Dateien in der Dropbox\n",
    "                    ut.running_bar(n,n_files)                                           # Plotte die Fortschrittsanzeige\n",
    "                    try:                                                                # Fange Exceptions ein und überspringe damit fehlerhafte Dateien\n",
    "                        test = load_tf_file(file, nrows=1)                              # Lade die erste Zeile jedes Sheets\n",
    "                        if test.index.isin(master_df[bui].index) == False:              # Prüfe ob diese Zeile bereits in der bestehenden Datenbank vorhanden ist.\n",
    "                            newdate = pd.to_datetime(test.index.values.min()).date()    # Wenn es sich um eine neue Zeile handelt, extrahiere das Datum von dem die Aufzeichnungen stammen\n",
    "                            log.info(f'--- {newdate}: Neue Datei gefunden!')            # Plotte eine kurze Info\n",
    "                            newday = load_tf_file(file)                                 # Lade den kompletten Sheet\n",
    "                            for sensor, data in newday.iteritems():                     # Für den neuen Sheet: Gehe Spalte für Spalte durch und zähle die Fehlerhaften Beobachtungen\n",
    "                                check = data.isna().sum()/len(data.index)               # Berechne den Anteil der fehlenden Messpunkte\n",
    "                                if check > 0.1:                                         # Wenn mehr als 10% Datenpunkte fehlen sollten\n",
    "                                    if bui not in notification:                         # Erstelle eine Benachrichtigung...\n",
    "                                        notification[bui] = {}\n",
    "                                    if sensor in notification[bui]:\n",
    "                                        notification[bui][sensor][newdate] = int((1-check)*100)     # Dazu: Berechne den Anteil an fehlenden Punkten und speichere ihn in einem Dict\n",
    "                                    else:\n",
    "                                        notification[bui][sensor] = {newdate : int((1-check)*100)}\n",
    "                            df1.append(newday)\n",
    "                    except Exception as e:                                              # Speichere die Exception im log....\n",
    "                        log.warning(e)\n",
    "                master_df[bui] = pd.concat([master_df[bui]] + df1,axis=0)               # Nachdem alle Datensheets überprüft wurden, hänge die neuen Datensätze der Datenbank an\n",
    "                master_df[bui].to_csv(db[bui], index=True)                              # exportiere die aktualisierte (Roh-)Datenbank\n",
    "                \n",
    "            elif not os.path.isfile(db[bui]) or bui in OverwriteDatabase:               # Falls die Bedingung vom Eingang nicht zutrifft, sprich: Entweder wurde keine Datenbank gefunden oder die Datenbank SOLL überschrieben werden\n",
    "                log.info(f'{bui}: Keine Datenbank gefunden - erstelle neue aus dem Archiv.')\n",
    "                bui_path = os.path.join(tf_archive, bui)                                # Konstruiere den Pfad zum Ordner der entsprechenden Einheit im Archiv-Ordner\n",
    "                if os.path.exists(bui_path):                                            # Wenn der Archivordner existiert (...tut er z.B. für das Pyranometer nicht...)\n",
    "                    files[bui] = [os.path.join(bui_path, fn) for fn in os.listdir(bui_path) if fn.endswith('.csv')]    # Speichere die Dateipfade zu den Datensätzen im einem Dict ab.\n",
    "                else:\n",
    "                    files[bui] = []                                                     # Wenn das Archiv nicht existiert erstelle wenigstens eine leere Liste\n",
    "                files[bui].extend([os.path.join(tf_drop_bui, fn) for fn in os.listdir(tf_drop_bui) if fn.endswith(f'{bui}.csv')])   # Nun gehe in den Dropbox Ordner und hänge alle Datensheets der Liste an. Achtung: nur Dateien die auf {bui}.csv enden werden berücksichtigt. Damit werden die \"Test\" Dateien in der Dropbox direkt aussortiert\n",
    "                files[bui].sort(key=lambda x: os.path.getmtime(x))                      # Sortiere alle Datasheets nach dem Datum.\n",
    "                # files[bui].pop(-1)                                                      # Zum testen: entferne den neusten Datensatz... Kann deaktiviert werden\n",
    "                n_files = len(files[bui])                                               # Zähle die Sheets (Für die Fortschrittsanzeige)\n",
    "                dfs = []                                                                # Erstelle eine Leere Liste in die die Datensheets abgespeichert werden\n",
    "                for nf, file in enumerate(files[bui]):                                  # Gehe alle Datensheets durch\n",
    "                    try:                                                                # Fange FileNotFound Exceptions ein um nicht jedes mal von neuem anfangen zu müssen, falls eine Datei nicht gefunden wurden....\n",
    "                        df = load_tf_file(file)                                         # Öffne den Datensheet\n",
    "                        if isinstance(df, pd.DataFrame):                                # Die Funktion load_tf_file() gibt None zurück, wenn die Datei z.B. Leer ist. Nur wenn ein pd.DataFrame geladen wurde, soll dieser der Liste hinzugefügt werden\n",
    "                            dfs.append(df)\n",
    "                    except FileNotFoundError:\n",
    "                        print(f'{file} wurde nicht geladen.')\n",
    "                    ut.running_bar(nf,n_files)                                          # Fortschrittsanzeige\n",
    "\n",
    "                df = pd.concat(dfs)                                                     # Vereine alle Datensheets zu einem großen\n",
    "                log.info(f'{bui}: Lesen der Datensätze erfolgreich. Exportiere die Datenbank.')\n",
    "                if bui == 'LB':                                                         # In der LB-Datei waren am Anfang auch die Pyranometer-Daten. Diese müssen extrahiert werden.\n",
    "                    master_df['PM'] = df.filter(like='DA').copy()                       # Dazu wird im master_df-dict eine Kopie der Pyranometer-Daten gespeichert \n",
    "                    master_df['PM'].columns = [col.rsplit('_',1)[-1] for col in master_df['PM'].columns]    # Deren Spaltenbezeichnugn wird noch vereinheitlicht\n",
    "                    master_df[bui] = df.drop(df.filter(like='DA').columns,axis=1).copy()    # Am Ende werden die Einträge der Pyranometer noch aus dem Bauweisen-dict gelöscht\n",
    "                elif bui == 'PM':                                                       # Für das Pyranometer wird das ganze jetzt andersrum gemacht. Erst werden die aus den LB-Daten extrahierten Datensätze geöffnet und die neuen Datensätze aus der Dropbox werden hinzugefügt.\n",
    "                    if bui in master_df and isinstance(master_df[bui], pd.DataFrame):   # Prüfe ob Daten aus dem Import eines LB-Sheets vorhanden sind.\n",
    "                        master_df[bui] = pd.concat([master_df[bui],df], axis = 0)       # Wenn dem so ist, dann füge Vereine sie mit dem neuen Datensatz\n",
    "                    else:\n",
    "                        master_df[bui] = df                                             # Wenn nicht, dann lade einfach nur den neuen Datensatz\n",
    "                else:\n",
    "                    master_df[bui] = df.copy()                                          # für alle anderen Datensätze: Speichere die neu geladenen Datensätze in dem master_df-dict\n",
    "                \n",
    "                master_df[bui].to_csv(db[bui], index=True)                              # und exportiere die Rohdaten als csv Datei\n",
    "                log.info(f'{bui}: Export der Datenbank erfolgreich.')\n",
    "                                                                                        # Hier beginnt das PostProcessing\n",
    "            master_df[bui].index = pd.to_datetime(master_df[bui].index)                 # Forme zuerst alle Indizes in das Datetime Format um.\n",
    "            if not master_df[bui].index.max().date() == dt.date.today():                # Nun Prüfe ob alle Datenbanken Aktuell sind, wenn nicht\n",
    "                log.warning(f'{bui}: Datenbank ist nicht aktuell. Vermutlich funktioniert der Dropbox-abgleich aktuell nicht.')\n",
    "            for name, data in master_df[bui].iteritems():                               # Nun forme alle numerischen Daten in Float-Werte um\n",
    "                master_df[bui][name] = pd.to_numeric(data, errors='ignore')\n",
    "            if not master_df[bui].columns.is_unique:                                    # Warne wenn duplizierte Spalten vorhanden sind\n",
    "                log.warning(f'{bui}: Duplizierte Spalten!')\n",
    "            start, ende = master_df[bui].index.min(), master_df[bui].index.max()        # Extrahiere das Start und Enddatum des gesamten Datensatzes                             \n",
    "            log.info(f'{bui}: Starte Export für den Datenzeitraum: {start} bis {ende}.')\n",
    "            allgood = True                                                              # Prüfwert, dass alle Datensätze korrekt exportiert wurden\n",
    "            for ts in ['1min', '15min', '60min']:                                       # Definiere die Zeitschritte für den Export\n",
    "                if os.path.isdir(os.path.join(dir_db,bui)) == False:                    # Falls der Export-Ordner nicht existieren sollte, erstelle ihn\n",
    "                    os.makedirs(os.path.join(dir_db,bui))\n",
    "                \n",
    "                df_resampled = master_df[bui].resample(ts).last().asfreq(ts)            # Führe Resampling auf den gewünschten Zeitschritt durch.\n",
    "                if df_resampled.index.is_unique:                                        # Wenn der Index jetzt keine Fehler mehr enthält, exportiere den Datensatz\n",
    "                    df_resampled.to_csv(os.path.join(dir_db,bui,'{}_tf_database_resampled_{}.csv'.format(bui, ts)))\n",
    "                    log.debug(f'{bui}: Resampled auf {ts}')\n",
    "                else:\n",
    "                    log.warning(f'{bui}: Resampling auf {ts} hat nicht geklappt! Es wurde nichts exportiert...')    # Falls Fehler enthalten sind, Schreibe eine Warnung ins Log \n",
    "                    allgood = False                                                     # Setze den Prüfwert auf Falsch\n",
    "            if allgood:                                                                 # Wenn alle exporte Geklappt haben, schreibe das ins log. 0\n",
    "                log.info(f'{bui}: Datenbanken exportiert!')\n",
    "\n",
    "    # send missing data report\n",
    "    for bui in list(notification):                                                      # Räume die Notifications auf.\n",
    "        for key in list(notification[bui]):\n",
    "            if 'named' in key:                                                          # Es kann passieren, dass die \"leeren\"-Spalten hier landen, die müssen erst aussortiert werden\n",
    "                notification[bui].pop(key)\n",
    "                log.warning(f'{bui}: unnamed column in data.')\n",
    "\n",
    "    if len(offline) == 0 and len(notification) == 0:                                    # Wenn nichts offline ist und alle Sensoren mehr als 90% Daten aufgezeichnet haben passt alles. Schreibe das ins Log und beende das Programm\n",
    "        log.info('Keine neuen Datensätze oder alles läuft einwandfrei...')\n",
    "    else:                                                                               # Wenn das nicht der Fall ist erstelle eine Benachrichtigungs-Mail\n",
    "                                                                                        # Beginne mit dem Einleitungstext der eMail    \n",
    "        text = 'Hallo,\\ndie folgenden Sensoren haben in den vergangenen Tagen ungewöhnlich wenige Daten aufgezeichnet:\\n\\nHinweis:\\nDie Liste führt Tage auf, an denen ein Sensor weniger als 90% der Datensätze aufgezeichnet hat. Fehlende Tage in der Aufzählung bedeuten, dass an diesen Tagen mehr als 90% der Daten aufgezeichnet wurden\\n'\n",
    "\n",
    "                                                                                        # Wenn alle Häuser \"Online\" sind - Sprich wir von allen Häusern aktuelle Datensheets haben, Schreibe das in die Mail\n",
    "        if len(offline)==0:\n",
    "            text += '\\n\\n---------ALLE HÄUSER ONLINE - DROPBOX-SYNC FUNKTIONIERT---------\\n\\n'\n",
    "                                        \n",
    "        for bui in buid:                                                                # Liste Haus für Haus, Tag für Tag die Fehlermeldungen auf\n",
    "            if bui in offline or bui in notification:\n",
    "                text +='\\n------'+ buid[bui] + ':------\\n'\n",
    "            if bui in offline:\n",
    "                text += '\\n!!! ' + offline[bui] + ' !!!\\n'\n",
    "            if bui in notification:\n",
    "                for sensor in notification[bui]:\n",
    "                    text += '\\n' + sensor + ':\\n'\n",
    "                    for day in notification[bui][sensor]:\n",
    "                        text += '\\t{}: {}% Messwerte vorhanden.\\n'.format(day, notification[bui][sensor][day])                                                                        \n",
    "            if send==False:                                                             # Wenn keine Mail versendet werden soll, schreibe die Infos ins Logbuch\n",
    "                log.debug(text)\n",
    "                                                                                        \n",
    "        if len(notification) > 0 and send==True:                                        # Wenn Benachrichtigungen da sind und sie auch versendet werden sollen, bereite die Mail vor\n",
    "                                                                                        \n",
    "            try:                                                                        # Der Versand der eMails erfolgt über eine vorgefertigte Funktion unter src.utilities. Darüber hinaus wird die config.py mit den Zugangsdaten zum eMail Provider benötigt.\n",
    "                ut.send_email(who=[config.emails['roman']],sender=config.emails['sender'],subject='EINFACH MESSEN: SENSORS NOT WORKING!',text=text,password=config.password,smtp=config.smtp)\n",
    "                log.info(f'Sensordaten-Übersicht gesendet')\n",
    "                                                # Schreibe was ins logbuch, wenn es nicht funktioniert hat\n",
    "            except Exception as e:\n",
    "                log.warning(f'Senden der Nachricht fehlgeschlagen: {e}')\n",
    "\n",
    "    # Versnde eine Extra Mail, wenn Häuser offline sind.            \n",
    "    if len(offline) == 0:\n",
    "        log.info('Keine neuen Datensätze oder alles läuft einwandfrei...')\n",
    "    else:\n",
    "        text = 'Hallo,\\ndas ist eine automatisch erzeugte Fehlermeldung der Einfach-Bauen-Haeuser:\\n\\n'\n",
    "\n",
    "        for bui in offline:\n",
    "            text += '-> Das/Die ' + offline[bui] + '\\n'\n",
    "\n",
    "        if send==False:\n",
    "            log.debug(text)\n",
    "\n",
    "        if len(offline) > 0 and send==True:\n",
    "            try:\n",
    "                ut.send_email(who=[config.emails['roman']],sender=config.emails['sender'],subject='EINFACH MESSEN: OFFLINE ALERT!',text=text,password=config.password,smtp=config.smtp)\n",
    "                log.info(f'Offline Benachrichtigung gesendet!')\n",
    "            except Exception as e:\n",
    "                log.warning(f'Senden der Nachricht fehlgeschlagen: {e}')\n",
    "\n",
    "    log.info('TinkerForge: FINISHED')                                           # Schreibe ins Log, dass das Skript druchgelaufen ist. !Achtung: Nach diesem Eintrag wird entschieden, ob ein Datenbank update notwendig ist oder nicht. Wenn dieser Eintrag mit heutigem Datum im Log steht, wird die DAtenbank nicht aktualisiert, wenn das nicht der Fall ist, wird ein Update angestoßen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.02.2022 18:12:38 -- INFO -- ------ Starte TinkerForge ------\n",
      "21.02.2022 18:12:38 -- INFO -- Suche vorhandene Datenbank.\n",
      "21.02.2022 18:12:47 -- INFO -- LB: Vorhandene Datenbank geöffnet!\n",
      "21.02.2022 18:12:47 -- INFO -- LB: Durchsuche Dropbox nach neuen Datensätzen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!                                                                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.02.2022 18:13:49 -- INFO -- --- 2022-02-20: Neue Datei gefunden!\n",
      "21.02.2022 18:14:29 -- INFO -- LB: Starte Export für den Datenzeitraum: 2021-03-08 11:15:55 bis 2022-02-21 00:08:50.\n",
      "21.02.2022 18:15:20 -- INFO -- LB: Datenbanken exportiert!\n",
      "21.02.2022 18:15:30 -- INFO -- MW: Vorhandene Datenbank geöffnet!\n",
      "21.02.2022 18:15:31 -- INFO -- Ziegelhaus liefert seit 17.02.2022 (4 Tage(n)) keine neuen Daten mehr.\n",
      "21.02.2022 18:15:31 -- INFO -- MW: Durchsuche Dropbox nach neuen Datensätzen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!                                                                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.02.2022 18:17:31 -- WARNING -- MW: Datenbank ist nicht aktuell. Vermutlich funktioniert der Dropbox-abgleich aktuell nicht.\n",
      "21.02.2022 18:17:34 -- INFO -- MW: Starte Export für den Datenzeitraum: 2021-02-23 17:19:00 bis 2022-02-18 00:07:27.\n",
      "21.02.2022 18:18:20 -- INFO -- MW: Datenbanken exportiert!\n",
      "21.02.2022 18:18:32 -- INFO -- MH: Vorhandene Datenbank geöffnet!\n",
      "21.02.2022 18:18:32 -- INFO -- MH: Durchsuche Dropbox nach neuen Datensätzen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!                                                                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.02.2022 18:20:37 -- INFO -- MH: Starte Export für den Datenzeitraum: 2021-02-08 11:00:51 bis 2022-02-21 00:08:00.\n",
      "21.02.2022 18:21:21 -- INFO -- MH: Datenbanken exportiert!\n",
      "21.02.2022 18:21:22 -- INFO -- PM: Vorhandene Datenbank geöffnet!\n",
      "21.02.2022 18:21:22 -- INFO -- PM: Durchsuche Dropbox nach neuen Datensätzen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!                                                                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.02.2022 18:22:42 -- INFO -- PM: Starte Export für den Datenzeitraum: 2021-03-08 11:15:55 bis 2022-02-21 00:08:50.\n",
      "21.02.2022 18:22:44 -- INFO -- PM: Datenbanken exportiert!\n",
      "21.02.2022 18:22:45 -- INFO -- WD: Vorhandene Datenbank geöffnet!\n",
      "21.02.2022 18:22:45 -- INFO -- WD: Durchsuche Dropbox nach neuen Datensätzen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!                                                                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.02.2022 18:23:44 -- INFO -- WD: Starte Export für den Datenzeitraum: 2021-03-08 11:18:55 bis 2022-02-21 00:08:50.\n",
      "21.02.2022 18:23:47 -- INFO -- WD: Datenbanken exportiert!\n",
      "21.02.2022 18:23:47 -- INFO -- Keine neuen Datensätze oder alles läuft einwandfrei...\n",
      "21.02.2022 18:23:47 -- INFO -- Keine neuen Datensätze oder alles läuft einwandfrei...\n",
      "21.02.2022 18:23:47 -- INFO -- TinkerForge: FINISHED\n"
     ]
    }
   ],
   "source": [
    "tinkerforge_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logpath = './logs'\n",
    "onlyfiles = [os.path.join(logpath, f) for f in os.listdir(logpath) if os.path.isfile(os.path.join(logpath, f))]\n",
    "onlyfiles.sort(key=lambda x: os.path.getmtime(x))\n",
    "for logfile in onlyfiles:\n",
    "    with open(logfile,encoding='latin-1') as f:\n",
    "        f = f.read().splitlines()\n",
    "        data = []\n",
    "        for line in f:\n",
    "            line = line.split(' -- ')\n",
    "            if line[1] == 'INFO' and (line[2] == '------ EnergyMeter finished! ------'or line[2] == 'EnergyMeter: FINISHED'):\n",
    "                Stand_EnergyMeter = dt.datetime.strptime(line[0], '%d.%m.%Y %H:%M:%S')\n",
    "            if line[1] == 'INFO' and (line[2] == 'TinkerForge Update komplett!' or line[2] == 'TinkerForge: FINISHED'):\n",
    "                Stand_TinkerForge = dt.datetime.strptime(line[0], '%d.%m.%Y %H:%M:%S')\n",
    "if Stand_TinkerForge.date() == dt.date.today():\n",
    "    print('Tinkerforge up-to-date')\n",
    "else:\n",
    "    tinkerforge_update()\n",
    "\n",
    "if Stand_EnergyMeter.date() == dt.date.today():\n",
    "    print('EnergyMeter up-to-date')\n",
    "else:\n",
    "    energymeter()\n",
    "    print('EnergyMeter Nessesarry')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59a70696c240545b1a70ae46b684ee7c051e707166acaec439211cc0efe87dd8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('einfach-bauen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
